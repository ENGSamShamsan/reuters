{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 165003,
     "status": "ok",
     "timestamp": 1544276516947,
     "user": {
      "displayName": "lololo jjjicjd",
      "photoUrl": "",
      "userId": "11180684639243280449"
     },
     "user_tz": 0
    },
    "id": "nqPvD_yK_22r",
    "outputId": "9f6b9df5-8f78-43b7-f24c-625174953ba0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [02:04, 6.93MB/s]                           \n",
      " 99%|█████████▉| 397710/400000 [00:12<00:00, 30392.10it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "data_path = 'input/reuters_small.pkl'\n",
    "labels_path = 'input/classcodes.csv'\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\"\"\"\n",
    "def listToInt(mylist, dict_):\n",
    "    return [dict_[item] for item in mylist]\n",
    "\n",
    "def multihot(tags, taglist):\n",
    "    return [1 if tag in tags else 0 for tag in taglist]\n",
    "\n",
    "reuters = pd.read_pickle(data_path)\n",
    "# read classcodes\n",
    "classcodes= pd.read_csv(labels_path)\n",
    "codetoi = dict(zip(classcodes.Code, range(len(classcodes))))\n",
    "# convert classcodes to int\n",
    "reuters['codes'] = [listToInt(codelist, codetoi) for codelist in reuters.codes]\n",
    "# convert classcodes to multihot\n",
    "reuters['codes'] = [multihot(claslist, range(126)) for claslist in reuters.codes]\n",
    "\n",
    "# construct train/test/val sets\n",
    "train = reuters[0:2500]\n",
    "test = reuters[2500:3000]\n",
    "val = reuters[3000:len(reuters)]\n",
    "# train.to_json('input/train.json', orient='records', lines=True)\n",
    "# test.to_json('input/test.json', orient='records', lines=True)\n",
    "# val.to_json('input/val.json', orient='records', lines=True)\n",
    "################################################################################\n",
    "\"\"\"\n",
    "train = 'input/train.json'\n",
    "valid = 'input/val.json'\n",
    "test = 'input/test.json'\n",
    "\n",
    "#Define the Fields\n",
    "TEXT = data.Field()\n",
    "HEADLINE = data.Field()\n",
    "LABELS = data.LabelField(sequential=False, use_vocab=False)\n",
    "fields = {'headline': ('h', HEADLINE), 'text': ('t', TEXT), 'codes': ('l', LABELS)}\n",
    "\n",
    "# Create dataset (TabularDataset)\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "                                        path = '',\n",
    "                                        train = train,\n",
    "                                        validation = valid,\n",
    "                                        test = test,\n",
    "                                        format = 'json',\n",
    "                                        fields = fields\n",
    ")\n",
    "\n",
    "# Without or with GLOVE\n",
    "TEXT.build_vocab(train_data, max_size=25000, vectors=\"glove.6B.50d\")\n",
    "HEADLINE.build_vocab(train)\n",
    "LABELS.build_vocab(train)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "(train_data, valid_data, test_data), \n",
    "batch_size=BATCH_SIZE,\n",
    "device=device,\n",
    "sort_key= lambda x: len(x.t)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1774
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 262674,
     "status": "ok",
     "timestamp": 1544283004399,
     "user": {
      "displayName": "lololo jjjicjd",
      "photoUrl": "",
      "userId": "11180684639243280449"
     },
     "user_tz": 0
    },
    "id": "FRSLH7CSD9Tq",
    "outputId": "432c302a-9d57-4b86-fda0-e5f49b548deb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "| Ep:01 |Tr Loss:0.255 |Prec:0.059 |Rec:0.175 |f1:0.089 |Val prec:0.294 |Val rec:0.163 |Val f1:0.209 |\n",
      "1\n",
      "| Ep:02 |Tr Loss:0.100 |Prec:0.392 |Rec:0.127 |f1:0.192 |Val prec:0.400 |Val rec:0.139 |Val f1:0.207 |\n",
      "2\n",
      "| Ep:03 |Tr Loss:0.090 |Prec:0.500 |Rec:0.127 |f1:0.203 |Val prec:0.516 |Val rec:0.123 |Val f1:0.199 |\n",
      "3\n",
      "| Ep:04 |Tr Loss:0.085 |Prec:0.635 |Rec:0.159 |f1:0.254 |Val prec:0.588 |Val rec:0.150 |Val f1:0.239 |\n",
      "4\n",
      "| Ep:05 |Tr Loss:0.078 |Prec:0.781 |Rec:0.251 |f1:0.380 |Val prec:0.709 |Val rec:0.159 |Val f1:0.259 |\n",
      "5\n",
      "| Ep:06 |Tr Loss:0.070 |Prec:0.840 |Rec:0.329 |f1:0.473 |Val prec:0.676 |Val rec:0.256 |Val f1:0.372 |\n",
      "6\n",
      "| Ep:07 |Tr Loss:0.065 |Prec:0.824 |Rec:0.369 |f1:0.510 |Val prec:0.733 |Val rec:0.281 |Val f1:0.406 |\n",
      "7\n",
      "| Ep:08 |Tr Loss:0.060 |Prec:0.848 |Rec:0.394 |f1:0.538 |Val prec:0.735 |Val rec:0.316 |Val f1:0.442 |\n",
      "8\n",
      "| Ep:09 |Tr Loss:0.056 |Prec:0.849 |Rec:0.423 |f1:0.565 |Val prec:0.736 |Val rec:0.321 |Val f1:0.447 |\n",
      "9\n",
      "| Ep:10 |Tr Loss:0.054 |Prec:0.876 |Rec:0.443 |f1:0.589 |Val prec:0.748 |Val rec:0.334 |Val f1:0.462 |\n",
      "10\n",
      "| Ep:11 |Tr Loss:0.051 |Prec:0.878 |Rec:0.465 |f1:0.608 |Val prec:0.741 |Val rec:0.338 |Val f1:0.465 |\n",
      "11\n",
      "| Ep:12 |Tr Loss:0.048 |Prec:0.887 |Rec:0.485 |f1:0.627 |Val prec:0.762 |Val rec:0.344 |Val f1:0.474 |\n",
      "12\n",
      "| Ep:13 |Tr Loss:0.045 |Prec:0.896 |Rec:0.510 |f1:0.650 |Val prec:0.747 |Val rec:0.363 |Val f1:0.489 |\n",
      "13\n",
      "| Ep:14 |Tr Loss:0.043 |Prec:0.900 |Rec:0.528 |f1:0.665 |Val prec:0.714 |Val rec:0.353 |Val f1:0.473 |\n",
      "14\n",
      "| Ep:15 |Tr Loss:0.041 |Prec:0.892 |Rec:0.536 |f1:0.670 |Val prec:0.758 |Val rec:0.369 |Val f1:0.497 |\n",
      "15\n",
      "| Ep:16 |Tr Loss:0.040 |Prec:0.900 |Rec:0.553 |f1:0.685 |Val prec:0.724 |Val rec:0.397 |Val f1:0.513 |\n",
      "16\n",
      "| Ep:17 |Tr Loss:0.038 |Prec:0.905 |Rec:0.571 |f1:0.700 |Val prec:0.754 |Val rec:0.402 |Val f1:0.525 |\n",
      "17\n",
      "| Ep:18 |Tr Loss:0.037 |Prec:0.906 |Rec:0.582 |f1:0.709 |Val prec:0.746 |Val rec:0.395 |Val f1:0.516 |\n",
      "18\n",
      "| Ep:19 |Tr Loss:0.035 |Prec:0.913 |Rec:0.594 |f1:0.720 |Val prec:0.733 |Val rec:0.418 |Val f1:0.532 |\n",
      "19\n",
      "| Ep:20 |Tr Loss:0.034 |Prec:0.909 |Rec:0.610 |f1:0.730 |Val prec:0.694 |Val rec:0.430 |Val f1:0.531 |\n",
      "20\n",
      "| Ep:21 |Tr Loss:0.033 |Prec:0.910 |Rec:0.621 |f1:0.738 |Val prec:0.695 |Val rec:0.431 |Val f1:0.533 |\n",
      "21\n",
      "| Ep:22 |Tr Loss:0.032 |Prec:0.906 |Rec:0.629 |f1:0.742 |Val prec:0.725 |Val rec:0.421 |Val f1:0.532 |\n",
      "22\n",
      "| Ep:23 |Tr Loss:0.031 |Prec:0.916 |Rec:0.633 |f1:0.748 |Val prec:0.731 |Val rec:0.423 |Val f1:0.536 |\n",
      "23\n",
      "| Ep:24 |Tr Loss:0.030 |Prec:0.919 |Rec:0.657 |f1:0.766 |Val prec:0.716 |Val rec:0.440 |Val f1:0.545 |\n",
      "24\n",
      "| Ep:25 |Tr Loss:0.030 |Prec:0.921 |Rec:0.659 |f1:0.768 |Val prec:0.704 |Val rec:0.446 |Val f1:0.546 |\n",
      "25\n",
      "| Ep:26 |Tr Loss:0.028 |Prec:0.910 |Rec:0.682 |f1:0.779 |Val prec:0.719 |Val rec:0.455 |Val f1:0.558 |\n",
      "26\n",
      "| Ep:27 |Tr Loss:0.027 |Prec:0.919 |Rec:0.688 |f1:0.787 |Val prec:0.699 |Val rec:0.447 |Val f1:0.545 |\n",
      "27\n",
      "| Ep:28 |Tr Loss:0.026 |Prec:0.916 |Rec:0.706 |f1:0.797 |Val prec:0.689 |Val rec:0.473 |Val f1:0.561 |\n",
      "28\n",
      "| Ep:29 |Tr Loss:0.026 |Prec:0.911 |Rec:0.709 |f1:0.797 |Val prec:0.693 |Val rec:0.463 |Val f1:0.555 |\n",
      "29\n",
      "| Ep:30 |Tr Loss:0.025 |Prec:0.920 |Rec:0.719 |f1:0.807 |Val prec:0.695 |Val rec:0.474 |Val f1:0.564 |\n",
      "30\n",
      "| Ep:31 |Tr Loss:0.024 |Prec:0.926 |Rec:0.726 |f1:0.814 |Val prec:0.674 |Val rec:0.466 |Val f1:0.551 |\n",
      "31\n",
      "| Ep:32 |Tr Loss:0.023 |Prec:0.921 |Rec:0.733 |f1:0.816 |Val prec:0.677 |Val rec:0.475 |Val f1:0.558 |\n",
      "32\n",
      "| Ep:33 |Tr Loss:0.022 |Prec:0.921 |Rec:0.747 |f1:0.825 |Val prec:0.689 |Val rec:0.512 |Val f1:0.588 |\n",
      "33\n",
      "| Ep:34 |Tr Loss:0.022 |Prec:0.926 |Rec:0.751 |f1:0.829 |Val prec:0.671 |Val rec:0.479 |Val f1:0.559 |\n",
      "34\n",
      "| Ep:35 |Tr Loss:0.021 |Prec:0.924 |Rec:0.764 |f1:0.837 |Val prec:0.696 |Val rec:0.472 |Val f1:0.562 |\n",
      "35\n",
      "| Ep:36 |Tr Loss:0.021 |Prec:0.924 |Rec:0.771 |f1:0.841 |Val prec:0.679 |Val rec:0.470 |Val f1:0.556 |\n",
      "36\n",
      "| Ep:37 |Tr Loss:0.020 |Prec:0.928 |Rec:0.776 |f1:0.845 |Val prec:0.687 |Val rec:0.482 |Val f1:0.566 |\n",
      "37\n",
      "| Ep:38 |Tr Loss:0.020 |Prec:0.932 |Rec:0.785 |f1:0.852 |Val prec:0.676 |Val rec:0.500 |Val f1:0.575 |\n",
      "38\n",
      "| Ep:39 |Tr Loss:0.020 |Prec:0.930 |Rec:0.783 |f1:0.850 |Val prec:0.673 |Val rec:0.493 |Val f1:0.569 |\n",
      "39\n",
      "| Ep:40 |Tr Loss:0.020 |Prec:0.928 |Rec:0.788 |f1:0.852 |Val prec:0.673 |Val rec:0.504 |Val f1:0.576 |\n",
      "40\n",
      "| Ep:41 |Tr Loss:0.019 |Prec:0.927 |Rec:0.794 |f1:0.855 |Val prec:0.681 |Val rec:0.488 |Val f1:0.569 |\n",
      "41\n",
      "| Ep:42 |Tr Loss:0.018 |Prec:0.931 |Rec:0.807 |f1:0.865 |Val prec:0.657 |Val rec:0.479 |Val f1:0.554 |\n",
      "42\n",
      "| Ep:43 |Tr Loss:0.017 |Prec:0.934 |Rec:0.810 |f1:0.867 |Val prec:0.659 |Val rec:0.505 |Val f1:0.572 |\n",
      "43\n",
      "| Ep:44 |Tr Loss:0.017 |Prec:0.933 |Rec:0.820 |f1:0.873 |Val prec:0.685 |Val rec:0.479 |Val f1:0.564 |\n",
      "44\n",
      "| Ep:45 |Tr Loss:0.017 |Prec:0.936 |Rec:0.817 |f1:0.873 |Val prec:0.675 |Val rec:0.503 |Val f1:0.577 |\n",
      "45\n",
      "| Ep:46 |Tr Loss:0.016 |Prec:0.935 |Rec:0.825 |f1:0.876 |Val prec:0.651 |Val rec:0.504 |Val f1:0.568 |\n",
      "46\n",
      "| Ep:47 |Tr Loss:0.016 |Prec:0.932 |Rec:0.823 |f1:0.874 |Val prec:0.665 |Val rec:0.503 |Val f1:0.573 |\n",
      "47\n",
      "| Ep:48 |Tr Loss:0.016 |Prec:0.934 |Rec:0.828 |f1:0.878 |Val prec:0.666 |Val rec:0.507 |Val f1:0.576 |\n",
      "48\n",
      "| Ep:49 |Tr Loss:0.016 |Prec:0.928 |Rec:0.833 |f1:0.878 |Val prec:0.660 |Val rec:0.506 |Val f1:0.573 |\n",
      "49\n",
      "| Ep:50 |Tr Loss:0.015 |Prec:0.932 |Rec:0.837 |f1:0.882 |Val prec:0.652 |Val rec:0.500 |Val f1:0.566 |\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def f1_own_accuracy(preds, y):\n",
    "    '''Returns counts of true_pos, false_pos and false_negative.\n",
    "    For counting precision, recall and F1 globally\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    '''\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    \n",
    "    preds = rounded_preds.cpu().data.numpy()\n",
    "    y = y.cpu().data.numpy()\n",
    "        \n",
    "    # True positive\n",
    "    tpos = np.sum(np.logical_and(preds == 1, y == 1))\n",
    " \n",
    "    # True negative\n",
    "    #tneg = np.sum(np.logical_and(preds == 0, y == 0))\n",
    " \n",
    "    # False positive\n",
    "    fpos = np.sum(np.logical_and(preds == 1, y == 0))\n",
    " \n",
    "    # False negative\n",
    "    fneg = np.sum(np.logical_and(preds == 0, y == 1))\n",
    "\n",
    "    return tpos, fpos, fneg\n",
    "  \n",
    "# F1 version\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    #epoch_acc = 0\n",
    "    epoch_tpos = 0\n",
    "    epoch_fpos = 0\n",
    "    epoch_fneg = 0    \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.t).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.l.float())\n",
    "            \n",
    "            tpos, fpos, fneg = f1_own_accuracy(predictions, batch.l.float())\n",
    "            epoch_tpos += tpos\n",
    "            epoch_fpos += fpos\n",
    "            epoch_fneg += fneg            \n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            #epoch_acc += acc.item()\n",
    "\n",
    "    # avoid div by zero with epsilon\n",
    "    eps = 1e-7\n",
    "    epoch_precision = epoch_tpos / (epoch_tpos + epoch_fpos +eps)\n",
    "    epoch_recall = epoch_tpos / (epoch_tpos + epoch_fneg +eps)\n",
    "    epoch_f1 = 2* (  (epoch_precision * epoch_recall) / (epoch_precision + epoch_recall +eps))            \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_precision, epoch_recall, epoch_f1\n",
    "\n",
    "\n",
    "# model\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 126\n",
    "N_EPOCHS = 50\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.em = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.conv4 = nn.Conv2d(1, 50, (5,50))\n",
    "        self.conv1 = nn.Conv2d(1, 50, (4,50))\n",
    "        self.conv2 = nn.Conv2d(1, 50, (3,50))\n",
    "        self.conv3 = nn.Conv2d(1, 50, (2,50))\n",
    "        self.fc1 = nn.Linear(50*4,150)\n",
    "        self.fc2 = nn.Linear(150, 126)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.em(x).permute(1,0,2).unsqueeze(1)\n",
    "        c1 = F.relu(self.conv1(x))\n",
    "        c2 = F.relu(self.conv2(x))\n",
    "        c3 = F.relu(self.conv3(x))\n",
    "        c4 = F.relu(self.conv4(x))\n",
    "        c1 = F.max_pool1d(c1.squeeze(3), c1.shape[2])\n",
    "        c2 = F.max_pool1d(c2.squeeze(3), c2.shape[2])\n",
    "        c3 = F.max_pool1d(c3.squeeze(3), c3.shape[2])\n",
    "        c4 = F.max_pool1d(c4.squeeze(3), c4.shape[2])\n",
    "        x = F.dropout(F.relu(self.fc1(torch.cat((c1, c2, c3, c4), dim=1).squeeze(2))))\n",
    "        output = self.fc2(x)\n",
    "        return output\n",
    "    \n",
    "model = Net(INPUT_DIM, EMBEDDING_DIM)\n",
    "model.em.weight.data.copy_(TEXT.vocab.vectors)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_tpos = 0\n",
    "    epoch_fpos = 0\n",
    "    epoch_fneg = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.t).squeeze(1)\n",
    "        loss = criterion(predictions, batch.l.float())\n",
    "        tpos, fpos, fneg = f1_own_accuracy(predictions, batch.l.float())\n",
    "        epoch_tpos += tpos\n",
    "        epoch_fpos += fpos\n",
    "        epoch_fneg += fneg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    eps = 1e-7\n",
    "    epoch_precision = epoch_tpos / (epoch_tpos + epoch_fpos +eps )\n",
    "    epoch_recall = epoch_tpos / (epoch_tpos + epoch_fneg +eps)\n",
    "    epoch_f1 = 2* (  (epoch_precision * epoch_recall) / (epoch_precision + epoch_recall +eps))\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_precision, epoch_recall, epoch_f1\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(epoch)\n",
    "    train_loss, train_precision, train_recall, train_f1 = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(model, valid_iterator, criterion)\n",
    "    #print(f'| Epoch:{epoch+1:02} | Train Loss: {train_loss:.3f} | Tr Precision: {train_precision:.3f} | Tr recall: {train_recall:.3f} | Tr f1: {train_f1:.3f} | Valid f1: {valid_f1:.3f} |')    \n",
    "    print(f'| Ep:{epoch+1:02} |Tr Loss:{train_loss:.3f} |Prec:{train_precision:.3f} |Rec:{train_recall:.3f} |f1:{train_f1:.3f} |Val prec:{valid_precision:.3f} |Val rec:{valid_recall:.3f} |Val f1:{valid_f1:.3f} |')     "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
