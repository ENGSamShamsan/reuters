{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision.datasets.utils import download_url\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3426\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>codes</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>classes</th>\n",
       "      <th>classes_pad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[C18, C181, CCAT]</td>\n",
       "      <td>Eureko is latest suitor for French insurer GAN.</td>\n",
       "      <td>\\nEureko, an alliance of six European financia...</td>\n",
       "      <td>[25, 26, 44]</td>\n",
       "      <td>[25, 26, 44, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[G15, GCAT]</td>\n",
       "      <td>Reuter EC Report Long-Term Diary for July 28 -...</td>\n",
       "      <td>\\n****\\nHIGHLIGHTS\\n****\\nLUXEMBOURG - Luxembo...</td>\n",
       "      <td>[80, 90]</td>\n",
       "      <td>[80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[G15, GCAT]</td>\n",
       "      <td>Official Journal contents - OJ L 190 of July 1...</td>\n",
       "      <td>\\n*\\n(Note - contents are displayed in reverse...</td>\n",
       "      <td>[80, 90]</td>\n",
       "      <td>[80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[G15, GCAT]</td>\n",
       "      <td>Official Journal contents - OJ C 221 of July 1...</td>\n",
       "      <td>\\n*\\n(Note - contents are displayed in reverse...</td>\n",
       "      <td>[80, 90]</td>\n",
       "      <td>[80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[G15, GCAT]</td>\n",
       "      <td>Official Journal contents - OJ C 220 of July 1...</td>\n",
       "      <td>\\n*\\n(Note - contents are displayed in reverse...</td>\n",
       "      <td>[80, 90]</td>\n",
       "      <td>[80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               codes                                           headline  \\\n",
       "0  [C18, C181, CCAT]    Eureko is latest suitor for French insurer GAN.   \n",
       "1        [G15, GCAT]  Reuter EC Report Long-Term Diary for July 28 -...   \n",
       "2        [G15, GCAT]  Official Journal contents - OJ L 190 of July 1...   \n",
       "3        [G15, GCAT]  Official Journal contents - OJ C 221 of July 1...   \n",
       "4        [G15, GCAT]  Official Journal contents - OJ C 220 of July 1...   \n",
       "\n",
       "                                                text       classes  \\\n",
       "0  \\nEureko, an alliance of six European financia...  [25, 26, 44]   \n",
       "1  \\n****\\nHIGHLIGHTS\\n****\\nLUXEMBOURG - Luxembo...      [80, 90]   \n",
       "2  \\n*\\n(Note - contents are displayed in reverse...      [80, 90]   \n",
       "3  \\n*\\n(Note - contents are displayed in reverse...      [80, 90]   \n",
       "4  \\n*\\n(Note - contents are displayed in reverse...      [80, 90]   \n",
       "\n",
       "                                         classes_pad  \n",
       "0  [25, 26, 44, -1, -1, -1, -1, -1, -1, -1, -1, -...  \n",
       "1   [80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  \n",
       "2   [80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  \n",
       "3   [80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  \n",
       "4   [80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data - short version 3400+ documents\n",
    "reuters = pd.read_pickle('input/reuters_small.pkl')\n",
    "print(len(reuters))\n",
    "reuters[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full data, if produced. 435 MB. NOT in Github currently. \n",
    "# You can produce this in about 30 min with preprocess_data.ipynb\n",
    "\n",
    "#reuters = pd.read_pickle('input/reuters_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n"
     ]
    }
   ],
   "source": [
    "# read classcodes\n",
    "classcodes= pd.read_csv('input/classcodes.csv')\n",
    "print(len(classcodes))\n",
    "#classcodes[0:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add index field to DataFrame\n",
    "classcodes = classcodes.reset_index()\n",
    "# Create dictionary index/int to classcode and classcode to int\n",
    "itocode = dict(zip(classcodes.index, classcodes.Code))\n",
    "codetoi = dict(zip(classcodes.Code, classcodes.index))\n",
    "def listToInt(mylist):\n",
    "    return [codetoi[item] for item in mylist]\n",
    "\n",
    "reuters['codes'] = [listToInt(codelist) for codelist in reuters.codes]\n",
    "reuters[0:3]\n",
    "# Multihot, for single list - one row\n",
    "def multihot(tags):\n",
    "    return [1 if tag in tags else 0 for tag in taglist]\n",
    "\n",
    "# list of classes, 126 int: [0...125]\n",
    "taglist = list(classcodes.index)\n",
    "Y_hot = [multihot(claslist) for claslist in reuters.codes]\n",
    "reuters['codes'] = Y_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>codes</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>classes</th>\n",
       "      <th>classes_pad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Eureko is latest suitor for French insurer GAN.</td>\n",
       "      <td>\\nEureko, an alliance of six European financia...</td>\n",
       "      <td>[25, 26, 44]</td>\n",
       "      <td>[25, 26, 44, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Reuter EC Report Long-Term Diary for July 28 -...</td>\n",
       "      <td>\\n****\\nHIGHLIGHTS\\n****\\nLUXEMBOURG - Luxembo...</td>\n",
       "      <td>[80, 90]</td>\n",
       "      <td>[80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Official Journal contents - OJ L 190 of July 1...</td>\n",
       "      <td>\\n*\\n(Note - contents are displayed in reverse...</td>\n",
       "      <td>[80, 90]</td>\n",
       "      <td>[80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Official Journal contents - OJ C 221 of July 1...</td>\n",
       "      <td>\\n*\\n(Note - contents are displayed in reverse...</td>\n",
       "      <td>[80, 90]</td>\n",
       "      <td>[80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Official Journal contents - OJ C 220 of July 1...</td>\n",
       "      <td>\\n*\\n(Note - contents are displayed in reverse...</td>\n",
       "      <td>[80, 90]</td>\n",
       "      <td>[80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               codes  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            headline  \\\n",
       "0    Eureko is latest suitor for French insurer GAN.   \n",
       "1  Reuter EC Report Long-Term Diary for July 28 -...   \n",
       "2  Official Journal contents - OJ L 190 of July 1...   \n",
       "3  Official Journal contents - OJ C 221 of July 1...   \n",
       "4  Official Journal contents - OJ C 220 of July 1...   \n",
       "\n",
       "                                                text       classes  \\\n",
       "0  \\nEureko, an alliance of six European financia...  [25, 26, 44]   \n",
       "1  \\n****\\nHIGHLIGHTS\\n****\\nLUXEMBOURG - Luxembo...      [80, 90]   \n",
       "2  \\n*\\n(Note - contents are displayed in reverse...      [80, 90]   \n",
       "3  \\n*\\n(Note - contents are displayed in reverse...      [80, 90]   \n",
       "4  \\n*\\n(Note - contents are displayed in reverse...      [80, 90]   \n",
       "\n",
       "                                         classes_pad  \n",
       "0  [25, 26, 44, -1, -1, -1, -1, -1, -1, -1, -1, -...  \n",
       "1   [80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  \n",
       "2   [80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  \n",
       "3   [80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  \n",
       "4   [80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using DataSet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example in:\n",
    "https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/A%20-%20Using%20TorchText%20with%20Your%20Own%20Datasets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "#Define the Fields\n",
    "TEXT = data.Field()\n",
    "HEADLINE = data.Field()\n",
    "\n",
    "LABELS = data.LabelField(sequential=False, use_vocab=False)\n",
    "#CLASSES = data.LabelField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>codes</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>classes</th>\n",
       "      <th>classes_pad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Eureko is latest suitor for French insurer GAN.</td>\n",
       "      <td>\\nEureko, an alliance of six European financia...</td>\n",
       "      <td>[25, 26, 44]</td>\n",
       "      <td>[25, 26, 44, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Reuter EC Report Long-Term Diary for July 28 -...</td>\n",
       "      <td>\\n****\\nHIGHLIGHTS\\n****\\nLUXEMBOURG - Luxembo...</td>\n",
       "      <td>[80, 90]</td>\n",
       "      <td>[80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Official Journal contents - OJ L 190 of July 1...</td>\n",
       "      <td>\\n*\\n(Note - contents are displayed in reverse...</td>\n",
       "      <td>[80, 90]</td>\n",
       "      <td>[80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Official Journal contents - OJ C 221 of July 1...</td>\n",
       "      <td>\\n*\\n(Note - contents are displayed in reverse...</td>\n",
       "      <td>[80, 90]</td>\n",
       "      <td>[80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Official Journal contents - OJ C 220 of July 1...</td>\n",
       "      <td>\\n*\\n(Note - contents are displayed in reverse...</td>\n",
       "      <td>[80, 90]</td>\n",
       "      <td>[80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               codes  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            headline  \\\n",
       "0    Eureko is latest suitor for French insurer GAN.   \n",
       "1  Reuter EC Report Long-Term Diary for July 28 -...   \n",
       "2  Official Journal contents - OJ L 190 of July 1...   \n",
       "3  Official Journal contents - OJ C 221 of July 1...   \n",
       "4  Official Journal contents - OJ C 220 of July 1...   \n",
       "\n",
       "                                                text       classes  \\\n",
       "0  \\nEureko, an alliance of six European financia...  [25, 26, 44]   \n",
       "1  \\n****\\nHIGHLIGHTS\\n****\\nLUXEMBOURG - Luxembo...      [80, 90]   \n",
       "2  \\n*\\n(Note - contents are displayed in reverse...      [80, 90]   \n",
       "3  \\n*\\n(Note - contents are displayed in reverse...      [80, 90]   \n",
       "4  \\n*\\n(Note - contents are displayed in reverse...      [80, 90]   \n",
       "\n",
       "                                         classes_pad  \n",
       "0  [25, 26, 44, -1, -1, -1, -1, -1, -1, -1, -1, -...  \n",
       "1   [80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  \n",
       "2   [80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  \n",
       "3   [80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  \n",
       "4   [80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3426"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test, save pd df to JSON\n",
    "                            # one record at time, one record by line\n",
    "reuters.to_json('input/reuters_small.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split it\n",
    "train = reuters[0:2500]\n",
    "test = reuters[2500:3000]\n",
    "val = reuters[3000:len(reuters)]\n",
    "\n",
    "train.to_json('input/train.json', orient='records', lines=True)\n",
    "test.to_json('input/test.json', orient='records', lines=True)\n",
    "val.to_json('input/val.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell torchText which Fields to apply to which json elements\n",
    "\n",
    "fields = {'headline': ('h', HEADLINE), 'text': ('t', TEXT), 'codes': ('l', LABELS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'headline': ('h', <torchtext.data.field.Field at 0x7f93602adf98>),\n",
       " 'text': ('t', <torchtext.data.field.Field at 0x7f93602ad9b0>),\n",
       " 'codes': ('l', <torchtext.data.field.LabelField at 0x7f93602ad748>)}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset (TabularDataset)\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "                                        path = 'input',\n",
    "                                        train = 'train.json',\n",
    "                                        validation = 'val.json',\n",
    "                                        test = 'test.json',\n",
    "                                        format = 'json',\n",
    "                                        fields = fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "#print(vars(train_data[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "device = torch.device('cpu')\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train, val, test), batch_sizes=(16, 256, 256),\n",
    "    sort_key=lambda x: len(x.text), device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without or with GLOVE\n",
    "\n",
    "#TEXT.build_vocab(train)\n",
    "TEXT.build_vocab(train_data, max_size=25000, vectors=\"glove.6B.50d\")\n",
    "HEADLINE.build_vocab(train)\n",
    "LABELS.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vocab at 0x7f936167c1d0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 2500\n",
      "Number of validation examples: 426\n",
      "Number of testing examples: 500\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, max_size=25000)\n",
    "LABELS.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABELS vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABELS vocabulary: {len(LABELS.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 22529), ('of', 12791), ('to', 12004), ('and', 9792), ('in', 9599), ('a', 8831), ('on', 5491), ('said', 4417), ('for', 4282), ('The', 3677)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', 'of', 'to', 'and', 'in', 'a', 'on', 'said']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x7f9366695158>, {0: 0, 1: 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABELS.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    sort_key= lambda x: len(x.t)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25002"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 126\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IF GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 50])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.4180,  0.2497, -0.4124,  ..., -0.1841, -0.1151, -0.7858],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# OR\n",
    "# optimizer = optim.adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results will be ranked according to the highest micro-averaged F1 score. \n",
    "This will be calculated using the f1_score function found in scikit-learn, using a command like \n",
    "f1_score(y_true, y_pred, average='micro') where y_true is the matrix with the ground truth, and y_pred \n",
    "the predicted output. Both matrices are binary, a 1 in row i and column j means that the image/document\n",
    "i contains the label j.\n",
    "\n",
    "Scikit:  Micro-average in F1-score\n",
    " \n",
    "'micro':\n",
    "    Calculate metrics globally by counting the total true positives, false negatives and false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 score for BATCH\n",
    "from sklearn.metrics import f1_score\n",
    "def f1_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns f1 accuracy from sklearn\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    \n",
    "    preds_cpu = rounded_preds.cpu().data.numpy()\n",
    "    y_cpu = y.cpu().data.numpy()\n",
    "    f1 = f1_score(y_cpu, preds_cpu, average='micro')\n",
    "    return f1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_own_accuracy(preds, y):\n",
    "    '''Returns counts of true_pos, false_pos and false_negative.\n",
    "    For counting precision, recall and F1 globally\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    '''\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    \n",
    "    preds = rounded_preds.cpu().data.numpy()\n",
    "    y = y.cpu().data.numpy()\n",
    "        \n",
    "    # True positive\n",
    "    tpos = np.sum(np.logical_and(preds == 1, y == 1))\n",
    " \n",
    "    # True negative\n",
    "    #tneg = np.sum(np.logical_and(preds == 0, y == 0))\n",
    " \n",
    "    # False positive\n",
    "    fpos = np.sum(np.logical_and(preds == 1, y == 0))\n",
    " \n",
    "    # False negative\n",
    "    fneg = np.sum(np.logical_and(preds == 0, y == 1))\n",
    "\n",
    "    return tpos, fpos, fneg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 version\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    #epoch_acc = 0\n",
    "    \n",
    "    epoch_tpos = 0\n",
    "    epoch_fpos = 0\n",
    "    epoch_fneg = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(batch.t).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.l.float())\n",
    "        \n",
    "        tpos, fpos, fneg = f1_own_accuracy(predictions, batch.l.float())\n",
    "        epoch_tpos += tpos\n",
    "        epoch_fpos += fpos\n",
    "        epoch_fneg += fneg\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        #epoch_acc += acc.item()\n",
    "        \n",
    "    epoch_precision = epoch_tpos / (epoch_tpos + epoch_fpos)\n",
    "    epoch_recall = epoch_tpos / (epoch_tpos + epoch_fneg)\n",
    "    epoch_f1 = 2* (  (epoch_precision * epoch_recall) / (epoch_precision + epoch_recall))\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_precision, epoch_recall, epoch_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 version\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    #epoch_acc = 0\n",
    "    epoch_tpos = 0\n",
    "    epoch_fpos = 0\n",
    "    epoch_fneg = 0    \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.t).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.l.float())\n",
    "            \n",
    "            tpos, fpos, fneg = f1_own_accuracy(predictions, batch.l.float())\n",
    "            epoch_tpos += tpos\n",
    "            epoch_fpos += fpos\n",
    "            epoch_fneg += fneg            \n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            #epoch_acc += acc.item()\n",
    "            \n",
    "    epoch_precision = epoch_tpos / (epoch_tpos + epoch_fpos)\n",
    "    epoch_recall = epoch_tpos / (epoch_tpos + epoch_fneg)\n",
    "    epoch_f1 = 2* (  (epoch_precision * epoch_recall) / (epoch_precision + epoch_recall))            \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_precision, epoch_recall, epoch_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "| Ep:01 |Tr Loss:0.692 |Prec:0.033 |Rec:0.615 |Tr f1:0.062 |Val prec:0.031 |Val rec:0.620 |Val f1:0.059 |\n",
      "1\n",
      "| Ep:02 |Tr Loss:0.691 |Prec:0.033 |Rec:0.609 |Tr f1:0.062 |Val prec:0.031 |Val rec:0.620 |Val f1:0.059 |\n",
      "2\n",
      "| Ep:03 |Tr Loss:0.691 |Prec:0.033 |Rec:0.609 |Tr f1:0.062 |Val prec:0.031 |Val rec:0.620 |Val f1:0.059 |\n",
      "3\n",
      "| Ep:04 |Tr Loss:0.691 |Prec:0.033 |Rec:0.609 |Tr f1:0.062 |Val prec:0.031 |Val rec:0.619 |Val f1:0.059 |\n"
     ]
    }
   ],
   "source": [
    "# Ver2 - F1\n",
    "N_EPOCHS = 4\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(epoch)\n",
    "\n",
    "    train_loss, train_precision, train_recall, train_f1 = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    #print(f'| Epoch:{epoch+1:02} | Train Loss: {train_loss:.3f} | Tr Precision: {train_precision:.3f} | Tr recall: {train_recall:.3f} | Tr f1: {train_f1:.3f} | Valid f1: {valid_f1:.3f} |')    \n",
    "    print(f'| Ep:{epoch+1:02} |Tr Loss:{train_loss:.3f} |Prec:{train_precision:.3f} |Rec:{train_recall:.3f} |f1:{train_f1:.3f} |Val prec:{valid_precision:.3f} |Val rec:{valid_recall:.3f} |Val f1:{valid_f1:.3f} |')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "| Ep:01 |Tr Loss:0.691 |Prec:0.033 |Rec:0.609 |Tr f1:0.062 |Val prec:0.031 |Val rec:0.618 |Val f1:0.059 |\n",
      "1\n",
      "| Ep:02 |Tr Loss:0.691 |Prec:0.032 |Rec:0.592 |Tr f1:0.060 |Val prec:0.028 |Val rec:0.556 |Val f1:0.054 |\n",
      "2\n",
      "| Ep:03 |Tr Loss:0.690 |Prec:0.030 |Rec:0.547 |Tr f1:0.057 |Val prec:0.029 |Val rec:0.554 |Val f1:0.054 |\n",
      "3\n",
      "| Ep:04 |Tr Loss:0.690 |Prec:0.030 |Rec:0.546 |Tr f1:0.057 |Val prec:0.029 |Val rec:0.554 |Val f1:0.055 |\n",
      "4\n",
      "| Ep:05 |Tr Loss:0.690 |Prec:0.031 |Rec:0.547 |Tr f1:0.058 |Val prec:0.029 |Val rec:0.553 |Val f1:0.055 |\n",
      "5\n",
      "| Ep:06 |Tr Loss:0.690 |Prec:0.031 |Rec:0.541 |Tr f1:0.060 |Val prec:0.030 |Val rec:0.543 |Val f1:0.057 |\n",
      "6\n",
      "| Ep:07 |Tr Loss:0.689 |Prec:0.032 |Rec:0.538 |Tr f1:0.060 |Val prec:0.030 |Val rec:0.543 |Val f1:0.057 |\n",
      "7\n",
      "| Ep:08 |Tr Loss:0.689 |Prec:0.032 |Rec:0.538 |Tr f1:0.060 |Val prec:0.030 |Val rec:0.541 |Val f1:0.057 |\n",
      "8\n",
      "| Ep:09 |Tr Loss:0.689 |Prec:0.032 |Rec:0.538 |Tr f1:0.060 |Val prec:0.030 |Val rec:0.540 |Val f1:0.056 |\n",
      "9\n",
      "| Ep:10 |Tr Loss:0.689 |Prec:0.032 |Rec:0.537 |Tr f1:0.060 |Val prec:0.030 |Val rec:0.538 |Val f1:0.056 |\n",
      "10\n",
      "| Ep:11 |Tr Loss:0.688 |Prec:0.032 |Rec:0.539 |Tr f1:0.060 |Val prec:0.030 |Val rec:0.538 |Val f1:0.056 |\n",
      "11\n",
      "| Ep:12 |Tr Loss:0.688 |Prec:0.032 |Rec:0.538 |Tr f1:0.060 |Val prec:0.030 |Val rec:0.538 |Val f1:0.056 |\n",
      "12\n",
      "| Ep:13 |Tr Loss:0.688 |Prec:0.032 |Rec:0.538 |Tr f1:0.060 |Val prec:0.030 |Val rec:0.536 |Val f1:0.057 |\n",
      "13\n",
      "| Ep:14 |Tr Loss:0.688 |Prec:0.032 |Rec:0.536 |Tr f1:0.061 |Val prec:0.031 |Val rec:0.536 |Val f1:0.058 |\n",
      "14\n",
      "| Ep:15 |Tr Loss:0.688 |Prec:0.033 |Rec:0.536 |Tr f1:0.062 |Val prec:0.031 |Val rec:0.535 |Val f1:0.058 |\n",
      "15\n",
      "| Ep:16 |Tr Loss:0.687 |Prec:0.033 |Rec:0.536 |Tr f1:0.062 |Val prec:0.031 |Val rec:0.534 |Val f1:0.058 |\n",
      "16\n",
      "| Ep:17 |Tr Loss:0.687 |Prec:0.033 |Rec:0.536 |Tr f1:0.062 |Val prec:0.031 |Val rec:0.534 |Val f1:0.058 |\n",
      "17\n",
      "| Ep:18 |Tr Loss:0.687 |Prec:0.033 |Rec:0.535 |Tr f1:0.063 |Val prec:0.031 |Val rec:0.533 |Val f1:0.059 |\n",
      "18\n",
      "| Ep:19 |Tr Loss:0.687 |Prec:0.033 |Rec:0.535 |Tr f1:0.063 |Val prec:0.031 |Val rec:0.531 |Val f1:0.059 |\n",
      "19\n",
      "| Ep:20 |Tr Loss:0.686 |Prec:0.033 |Rec:0.535 |Tr f1:0.063 |Val prec:0.031 |Val rec:0.531 |Val f1:0.059 |\n"
     ]
    }
   ],
   "source": [
    "# Ver2 - F1\n",
    "N_EPOCHS = 20\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(epoch)\n",
    "\n",
    "    train_loss, train_precision, train_recall, train_f1 = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    #print(f'| Epoch:{epoch+1:02} | Train Loss: {train_loss:.3f} | Tr Precision: {train_precision:.3f} | Tr recall: {train_recall:.3f} | Tr f1: {train_f1:.3f} | Valid f1: {valid_f1:.3f} |')    \n",
    "    print(f'| Ep:{epoch+1:02} |Tr Loss:{train_loss:.3f} |Prec:{train_precision:.3f} |Rec:{train_recall:.3f} |Tr f1:{train_f1:.3f} |Val prec:{valid_precision:.3f} |Val rec:{valid_recall:.3f} |Val f1:{valid_f1:.3f} |')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3x60 epochs, validation f1 0.247"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "| Ep:01 |Tr Loss:0.377 |Prec:0.511 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "1\n",
      "| Ep:02 |Tr Loss:0.374 |Prec:0.511 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "2\n",
      "| Ep:03 |Tr Loss:0.370 |Prec:0.511 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "3\n",
      "| Ep:04 |Tr Loss:0.367 |Prec:0.511 |Rec:0.156 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "4\n",
      "| Ep:05 |Tr Loss:0.363 |Prec:0.510 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "5\n",
      "| Ep:06 |Tr Loss:0.360 |Prec:0.511 |Rec:0.157 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "6\n",
      "| Ep:07 |Tr Loss:0.356 |Prec:0.510 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "7\n",
      "| Ep:08 |Tr Loss:0.353 |Prec:0.511 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "8\n",
      "| Ep:09 |Tr Loss:0.350 |Prec:0.510 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "9\n",
      "| Ep:10 |Tr Loss:0.347 |Prec:0.511 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "10\n",
      "| Ep:11 |Tr Loss:0.343 |Prec:0.510 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "11\n",
      "| Ep:12 |Tr Loss:0.340 |Prec:0.512 |Rec:0.156 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "12\n",
      "| Ep:13 |Tr Loss:0.337 |Prec:0.511 |Rec:0.156 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "13\n",
      "| Ep:14 |Tr Loss:0.334 |Prec:0.511 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "14\n",
      "| Ep:15 |Tr Loss:0.331 |Prec:0.511 |Rec:0.156 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "15\n",
      "| Ep:16 |Tr Loss:0.328 |Prec:0.511 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "16\n",
      "| Ep:17 |Tr Loss:0.325 |Prec:0.511 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "17\n",
      "| Ep:18 |Tr Loss:0.322 |Prec:0.511 |Rec:0.156 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "18\n",
      "| Ep:19 |Tr Loss:0.319 |Prec:0.511 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "19\n",
      "| Ep:20 |Tr Loss:0.316 |Prec:0.510 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "20\n",
      "| Ep:21 |Tr Loss:0.313 |Prec:0.511 |Rec:0.156 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "21\n",
      "| Ep:22 |Tr Loss:0.311 |Prec:0.510 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "22\n",
      "| Ep:23 |Tr Loss:0.308 |Prec:0.511 |Rec:0.156 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "23\n",
      "| Ep:24 |Tr Loss:0.305 |Prec:0.510 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "24\n",
      "| Ep:25 |Tr Loss:0.302 |Prec:0.511 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "25\n",
      "| Ep:26 |Tr Loss:0.300 |Prec:0.511 |Rec:0.156 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "26\n",
      "| Ep:27 |Tr Loss:0.298 |Prec:0.511 |Rec:0.156 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "27\n",
      "| Ep:28 |Tr Loss:0.295 |Prec:0.511 |Rec:0.156 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "28\n",
      "| Ep:29 |Tr Loss:0.292 |Prec:0.510 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "29\n",
      "| Ep:30 |Tr Loss:0.290 |Prec:0.511 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "30\n",
      "| Ep:31 |Tr Loss:0.287 |Prec:0.510 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "31\n",
      "| Ep:32 |Tr Loss:0.285 |Prec:0.511 |Rec:0.157 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "32\n",
      "| Ep:33 |Tr Loss:0.283 |Prec:0.510 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "33\n",
      "| Ep:34 |Tr Loss:0.281 |Prec:0.511 |Rec:0.156 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "34\n",
      "| Ep:35 |Tr Loss:0.278 |Prec:0.510 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "35\n",
      "| Ep:36 |Tr Loss:0.276 |Prec:0.510 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "36\n",
      "| Ep:37 |Tr Loss:0.274 |Prec:0.511 |Rec:0.157 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "37\n",
      "| Ep:38 |Tr Loss:0.272 |Prec:0.511 |Rec:0.156 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "38\n",
      "| Ep:39 |Tr Loss:0.269 |Prec:0.511 |Rec:0.157 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "39\n",
      "| Ep:40 |Tr Loss:0.267 |Prec:0.511 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "40\n",
      "| Ep:41 |Tr Loss:0.265 |Prec:0.510 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "41\n",
      "| Ep:42 |Tr Loss:0.263 |Prec:0.510 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "42\n",
      "| Ep:43 |Tr Loss:0.261 |Prec:0.511 |Rec:0.157 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "43\n",
      "| Ep:44 |Tr Loss:0.259 |Prec:0.510 |Rec:0.157 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "44\n",
      "| Ep:45 |Tr Loss:0.257 |Prec:0.511 |Rec:0.157 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "45\n",
      "| Ep:46 |Tr Loss:0.255 |Prec:0.511 |Rec:0.157 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "46\n",
      "| Ep:47 |Tr Loss:0.254 |Prec:0.510 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "47\n",
      "| Ep:48 |Tr Loss:0.251 |Prec:0.511 |Rec:0.157 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "48\n",
      "| Ep:49 |Tr Loss:0.250 |Prec:0.511 |Rec:0.156 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "49\n",
      "| Ep:50 |Tr Loss:0.248 |Prec:0.511 |Rec:0.157 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "50\n",
      "| Ep:51 |Tr Loss:0.246 |Prec:0.511 |Rec:0.157 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "51\n",
      "| Ep:52 |Tr Loss:0.244 |Prec:0.511 |Rec:0.156 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "52\n",
      "| Ep:53 |Tr Loss:0.242 |Prec:0.511 |Rec:0.157 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "53\n",
      "| Ep:54 |Tr Loss:0.241 |Prec:0.511 |Rec:0.157 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "54\n",
      "| Ep:55 |Tr Loss:0.239 |Prec:0.511 |Rec:0.156 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "55\n",
      "| Ep:56 |Tr Loss:0.238 |Prec:0.511 |Rec:0.157 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "56\n",
      "| Ep:57 |Tr Loss:0.236 |Prec:0.511 |Rec:0.157 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "57\n",
      "| Ep:58 |Tr Loss:0.234 |Prec:0.511 |Rec:0.157 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "58\n",
      "| Ep:59 |Tr Loss:0.233 |Prec:0.510 |Rec:0.156 |f1:0.239 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n",
      "59\n",
      "| Ep:60 |Tr Loss:0.231 |Prec:0.510 |Rec:0.157 |f1:0.240 |Val prec:0.499 |Val rec:0.164 |Val f1:0.247 |\n"
     ]
    }
   ],
   "source": [
    "# Ver2 - F1\n",
    "N_EPOCHS = 60\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(epoch)\n",
    "\n",
    "    train_loss, train_precision, train_recall, train_f1 = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    #print(f'| Epoch:{epoch+1:02} | Train Loss: {train_loss:.3f} | Tr Precision: {train_precision:.3f} | Tr recall: {train_recall:.3f} | Tr f1: {train_f1:.3f} | Valid f1: {valid_f1:.3f} |')    \n",
    "    print(f'| Ep:{epoch+1:02} |Tr Loss:{train_loss:.3f} |Prec:{train_precision:.3f} |Rec:{train_recall:.3f} |f1:{train_f1:.3f} |Val prec:{valid_precision:.3f} |Val rec:{valid_recall:.3f} |Val f1:{valid_f1:.3f} |')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading doesnt fully work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pkl')\n",
    "#torch.save(model, 'filename.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = RNN(input_dim=25002, embedding_dim=50, hidden_dim=256, output_dim=126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.load_state_dict(torch.load('model.pkl'))\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(25002, 50)\n",
       "  (rnn): RNN(50, 256)\n",
       "  (fc): Linear(in_features=256, out_features=126, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### easier but not so compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'modelx.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelx = torch.load('modelx.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test it on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.235 ||Prec:0.515 |Rec:0.144 |Tr F1: 0.225 |\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_precision, test_recall, test_f1 = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} ||Prec:{test_precision:.3f} |Rec:{test_recall:.3f} |Tr F1:{test_f1: .3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use final scoring \n",
    "\n",
    "The results will be ranked according to the highest micro-averaged F1 score. This will be calculated using the f1_score function found in scikit-learn, using a command like f1_score(y_true, y_pred, average='micro') where y_true is the matrix with the ground truth, and y_pred the predicted output. Both matrices are binary, a 1 in row i and column j means that the image/document i contains the label j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
