{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA20001 Deep Learning - Group Project\n",
    "## Text project\n",
    "\n",
    "**Due Thursday, December 13, before 23:59.**\n",
    "\n",
    "The task is to learn to assign the correct labels to news articles.  The corpus contains ~850K articles from Reuters.  The test set is about 10% of the articles. The data is unextracted in XML files.\n",
    "\n",
    "We're only giving you the code for downloading the data, and how to save the final model. The rest you'll have to do yourselves.\n",
    "\n",
    "Some comments and hints particular to the project:\n",
    "\n",
    "- One document may belong to many classes in this problem, i.e., it's a multi-label classification problem. In fact there are documents that don't belong to any class, and you should also be able to handle these correctly. Pay careful attention to how you design the outputs of the network (e.g., what activation to use) and what loss function should be used.\n",
    "- You may use word-embeddings to get better results. For example, you were already using a smaller version of the GloVE  embeddings in exercise 4. Do note that these embeddings take a lot of memory. \n",
    "- In the exercises we used e.g., `torchvision.datasets.MNIST` to handle the loading of the data in suitable batches. Here, you need to handle the dataloading yourself.  The easiest way is probably to create a custom `Dataset`. [See for example here for a tutorial](https://github.com/utkuozbulak/pytorch-custom-dataset-examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision.datasets.utils import download_url\n",
    "import zipfile\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_one_zipfile(filepath):  \n",
    "    '''\n",
    "    read and parse contents of single zipfile (with about 100+ xml-files in it)\n",
    "    fields: headline, text, classes\n",
    "    return them as list\n",
    "    '''\n",
    "    this_documents=[]\n",
    "    \n",
    "    zf = zipfile.ZipFile(filepath, 'r')    \n",
    "\n",
    "    # for all xml-files within a zip\n",
    "    for name in zf.namelist():\n",
    "        #if name.endswith('xml'): continue\n",
    "    \n",
    "        infile = zf.open(name)    \n",
    "        contents = infile.read()\n",
    "        soup = BeautifulSoup(contents,'lxml')\n",
    "    \n",
    "        headline = soup.find('headline')\n",
    "        text = soup.find('text')       #print(headline.get_text())\n",
    "    \n",
    "    # extract all topic-classes \n",
    "    # only take \"codes\" by topic, not region or industry: class == 'bip:topics:1.0'\n",
    "        classcodes = []\n",
    "        for element in soup.find_all('codes', class_='bip:topics:1.0'):\n",
    "            for code in element.find_all('code'):\n",
    "                clas = code['code']\n",
    "                #print(clas)\n",
    "                classcodes.append(clas)\n",
    "\n",
    "        this_documents.append({'headline': headline.get_text(), 'text': text.get_text(), 'codes': classcodes})\n",
    "    return this_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'train/'\n",
    "\n",
    "dl_file='reuters.zip'\n",
    "dl_url='https://www.cs.helsinki.fi/u/jgpyykko/'\n",
    "zip_path = os.path.join(train_path, dl_file)\n",
    "if not os.path.isfile(zip_path):\n",
    "    download_url(dl_url + dl_file, root=train_path, filename=dl_file, md5=None)\n",
    "\n",
    "with zipfile.ZipFile(zip_path) as zip_f:\n",
    "    zip_f.extractall(train_path)\n",
    "    #os.unlink(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command downloads and extracts the data files into the `train` subdirectory.\n",
    "\n",
    "The files can be found in `train/`, and are named as `19970405.zip`, etc. You will have to manage the content of these zips to get the data. There is a readme which has links to further descriptions on the data.\n",
    "\n",
    "The class labels, or topics, can be found in the readme file called `train/codes.zip`.  The zip contains a file called \"topic_codes.txt\".  This file contains the special codes for the topics (about 130 of them), and the explanation - what each code means.  \n",
    "\n",
    "The XML document files contain the article's headline, the main body text, and the list of topic labels assigned to each article.  You will have to extract the topics of each article from the XML.  For example: \n",
    "&lt;code code=\"C18\"&gt; refers to the topic \"OWNERSHIP CHANGES\" (like a corporate buyout).\n",
    "\n",
    "You should pre-process the XML to extract the words from the article: the &lt;headline&gt; element and the &lt;text&gt;.  You should not need any other parts of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the CLASS codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read one of the Class-code files\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "zf = zipfile.ZipFile('train/REUTERS_CORPUS_2/codes.zip', 'r') \n",
    "colnames=['Code','Description']\n",
    "df = pd.read_csv(zf.open('topic_codes.txt'), skiprows=2, error_bad_lines=True, \n",
    "                 header=None, names=colnames, sep='\\t')\n",
    "\n",
    "# df # (the file has 2 first rows as CODE/DESCRIPTION, the extra line is still at row 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "df.to_csv('input/classcodes.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "classcodes= pd.read_csv('input/classcodes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom bs4 import BeautifulSoup\\ninfile = open(\"train/477886newsML.xml\",\"r\")\\ncontents = infile.read()\\nsoup = BeautifulSoup(contents,\\'lxml\\') # use parser lxml as parser xml returns empty list\\n\\nheadline = soup.find(\\'headline\\')\\nprint(headline.get_text())\\n\\ntext = soup.find(\\'text\\')\\nprint(text.get_text()[0:1000])\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example http://www2.hawaii.edu/~takebaya/cent110/xml_parse/xml_parse.html\n",
    "\n",
    "# testing\n",
    "# unzip a single xml first\n",
    "\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "infile = open(\"train/477886newsML.xml\",\"r\")\n",
    "contents = infile.read()\n",
    "soup = BeautifulSoup(contents,'lxml') # use parser lxml as parser xml returns empty list\n",
    "\n",
    "headline = soup.find('headline')\n",
    "print(headline.get_text())\n",
    "\n",
    "text = soup.find('text')\n",
    "print(text.get_text()[0:1000])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codes = soup.find_all('code')\n",
    "# for code in codes:\n",
    "#     print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to topic_codes.txt inside codes.zip, Which defines that \n",
    "\n",
    "G15 EUROPEAN COMMUNITY \n",
    "\n",
    "GCAT\tGOVERNMENT/SOCIAL\n",
    "\n",
    "EEC is found in region_codes.txt: EEC\tEUROPEAN UNION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take codes by topic, not region or industry\n",
    "# codes = soup.find_all('codes', class_='bip:topics:1.0')\n",
    "# for code in codes:\n",
    "#     print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G15\n",
      "GCAT\n"
     ]
    }
   ],
   "source": [
    "# extract all topic-classes \n",
    "# only take \"codes\" by topic, not region or industry: class == 'bip:topics:1.0'\n",
    "# example\n",
    "# <codes class=\"bip:topics:1.0\">\n",
    "# <code code=\"G15\"> ... </code>\n",
    "# <code code=\"GCAT\"> ... </code>\n",
    "# </codes>\n",
    "for element in soup.find_all('codes', class_='bip:topics:1.0'):\n",
    "    for code in element.find_all('code'):\n",
    "        clas = code['code']\n",
    "        print(clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n",
      "['19970722.zip', '19970508.zip', '19970421.zip', '19970612.zip']\n"
     ]
    }
   ],
   "source": [
    "# get list of *.zip files in dir, such that contain xml-files (name starts with 1).\n",
    "dirpath = 'train/REUTERS_CORPUS_2/'\n",
    "files = [f for f in os.listdir(dirpath) if os.path.isfile(os.path.join(dirpath, f))]\n",
    "# cut out codes.zip, readme.txt etc. All zips containing .xml start with 1\n",
    "filenames_zip = [f for f in files if '1' in f]\n",
    "print(len(filenames_zip))\n",
    "print(filenames_zip[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get xml-filenames inside a single zip-file\n",
    "mypath = 'train/REUTERS_CORPUS_2/'\n",
    "file = '19970722.zip'\n",
    "zf = zipfile.ZipFile(mypath+file, 'r')\n",
    "\n",
    "# get names of all xml-files within a zip\n",
    "# for name in zf.namelist():    \n",
    "    # print(name)    \n",
    "    #f = zf.open(name)\n",
    "    #print(f.read()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3426"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read single zipfiles contents\n",
    "mypath = 'train/REUTERS_CORPUS_2/'\n",
    "documents= []    \n",
    "\n",
    "file = '19970722.zip'\n",
    "\n",
    "documents.extend( read_one_zipfile(mypath+file) )\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_small = pd.DataFrame(documents)\n",
    "# data_small[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional - faster test with cutting list to 2 zipfiles\n",
    "#filenames_zip = filenames_zip[0:2]\n",
    "#filenames_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299773"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CAN TAKE ABOUT 30-60 MIN!\n",
    "# Read all zipfiles\n",
    "mypath = 'train/REUTERS_CORPUS_2/'\n",
    "documents= []\n",
    "\n",
    "ind_8 = 0\n",
    "for i, file in enumerate(filenames_zip):\n",
    "    if i == 8:\n",
    "        ind_8 = len(documents)\n",
    "    documents.extend( read_one_zipfile(mypath+file) )\n",
    "len(documents)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(documents)\n",
    "# data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn classnames into integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### string-to-int and int-to-string dictionaries for classcodes, turn classes into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add index field to DataFrame\n",
    "classcodes = classcodes.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary index/int to classcode and classcode to int\n",
    "itocode = dict(zip(classcodes.index, classcodes.Code))\n",
    "codetoi = dict(zip(classcodes.Code, classcodes.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '1POL'),\n",
       " (1, '2ECO'),\n",
       " (2, '3SPO'),\n",
       " (3, '4GEN'),\n",
       " (4, '6INS'),\n",
       " (5, '7RSK'),\n",
       " (6, '8YDB')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itocode.items())[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4GEN\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(itocode[3])\n",
    "print(codetoi['4GEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 26, 44]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn one list of codes into ints\n",
    "def listToInt(mylist):\n",
    "    return [codetoi[item] for item in mylist]\n",
    "\n",
    "#test\n",
    "listToInt(['C18', 'C181', 'CCAT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               codes                                           headline  \\\n",
      "0  [C18, C181, CCAT]    Eureko is latest suitor for French insurer GAN.   \n",
      "1        [G15, GCAT]  Reuter EC Report Long-Term Diary for July 28 -...   \n",
      "2        [G15, GCAT]  Official Journal contents - OJ L 190 of July 1...   \n",
      "\n",
      "                                                text       classes  \n",
      "0  \\nEureko, an alliance of six European financia...  [25, 26, 44]  \n",
      "1  \\n****\\nHIGHLIGHTS\\n****\\nLUXEMBOURG - Luxembo...      [80, 90]  \n",
      "2  \\n*\\n(Note - contents are displayed in reverse...      [80, 90]  \n"
     ]
    }
   ],
   "source": [
    "# for each list of codes, turn it to ints\n",
    "reuters = data_small\n",
    "\n",
    "reuters['classes'] = [listToInt(codelist) for codelist in reuters.codes]\n",
    "data_small = reuters\n",
    "print(reuters[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters2 = pd.DataFrame(data.loc[:ind_8])\n",
    "reuters2['classes'] = [listToInt(codelist) for codelist in reuters2.codes]\n",
    "data_8 = reuters2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad with -1 for given length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " for pytorch nn.MultiLabelMarginLoss(), which expects labels in start, then -1 padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, '-1', '-1']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad list with -1 to given length\n",
    "def padList(mylist, length=10):\n",
    "\n",
    "    mylist = (mylist + length*['-1'])[:length]\n",
    "    return mylist\n",
    "\n",
    "#test\n",
    "padList([2,3],length=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>codes</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>classes</th>\n",
       "      <th>classes_pad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[C18, C181, CCAT]</td>\n",
       "      <td>Eureko is latest suitor for French insurer GAN.</td>\n",
       "      <td>\\nEureko, an alliance of six European financia...</td>\n",
       "      <td>[25, 26, 44]</td>\n",
       "      <td>[25, 26, 44, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[G15, GCAT]</td>\n",
       "      <td>Reuter EC Report Long-Term Diary for July 28 -...</td>\n",
       "      <td>\\n****\\nHIGHLIGHTS\\n****\\nLUXEMBOURG - Luxembo...</td>\n",
       "      <td>[80, 90]</td>\n",
       "      <td>[80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[G15, GCAT]</td>\n",
       "      <td>Official Journal contents - OJ L 190 of July 1...</td>\n",
       "      <td>\\n*\\n(Note - contents are displayed in reverse...</td>\n",
       "      <td>[80, 90]</td>\n",
       "      <td>[80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               codes                                           headline  \\\n",
       "0  [C18, C181, CCAT]    Eureko is latest suitor for French insurer GAN.   \n",
       "1        [G15, GCAT]  Reuter EC Report Long-Term Diary for July 28 -...   \n",
       "2        [G15, GCAT]  Official Journal contents - OJ L 190 of July 1...   \n",
       "\n",
       "                                                text       classes  \\\n",
       "0  \\nEureko, an alliance of six European financia...  [25, 26, 44]   \n",
       "1  \\n****\\nHIGHLIGHTS\\n****\\nLUXEMBOURG - Luxembo...      [80, 90]   \n",
       "2  \\n*\\n(Note - contents are displayed in reverse...      [80, 90]   \n",
       "\n",
       "                                         classes_pad  \n",
       "0  [25, 26, 44, -1, -1, -1, -1, -1, -1, -1, -1, -...  \n",
       "1   [80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  \n",
       "2   [80, 90, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each list of codes, pad it\n",
    "reuters = data_small\n",
    "\n",
    "reuters['classes_pad'] = padList(reuters['classes'], length=10)\n",
    "data_small = reuters\n",
    "reuters[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save small example-table to pickle\n",
    "data_small.to_pickle('input/reuters_small.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_small.to_json('input/reuters_small.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load small\n",
    "reuters = pd.read_pickle('input/reuters_small.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save 8 zip example-table to pickle\n",
    "data_small.to_pickle('input/reuters_small8.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save large table to pickle\n",
    "data.to_pickle('input/reuters_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load large\n",
    "reuters = pd.read_pickle('input/reuters_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read new unseen data: XML / ZIP, preprocess it and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "['19970410-test.zip', '19970619-test.zip', '19970719-test.zip', '19970510-test.zip']\n"
     ]
    }
   ],
   "source": [
    "# get list of *.zip files in dir, such that contain xml-files (name starts with 1).\n",
    "dirpath = 'test_data/'\n",
    "files = [f for f in os.listdir(dirpath) if os.path.isfile(os.path.join(dirpath, f))]\n",
    "# cut out codes.zip, readme.txt etc. All zips containing .xml start with 1\n",
    "# NOTICE - in new_data, all zips contain 19, also one extra html contain only 1\n",
    "filenames_zip = [f for f in files if '19' in f]\n",
    "print(len(filenames_zip))\n",
    "print(filenames_zip[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33142"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read all zipfiles\n",
    "mypath = 'test_data/'\n",
    "documents= []\n",
    "\n",
    "ind_8 = 0\n",
    "for i, file in enumerate(filenames_zip):\n",
    "    if i == 8:\n",
    "        ind_8 = len(documents)\n",
    "    documents.extend( read_one_zipfile(mypath+file) )\n",
    "len(documents)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>codes</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>PRESS DIGEST - SOUTH AFRICA - APRIL 10.</td>\n",
       "      <td>\\nThese are the leading stories in the South A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>OFFICIAL JOURNAL CONTENTS - OJ C 110 OF APRIL ...</td>\n",
       "      <td>\\n*\\n(Note - contents are displayed in reverse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  codes                                           headline  \\\n",
       "0    []            PRESS DIGEST - SOUTH AFRICA - APRIL 10.   \n",
       "1    []  OFFICIAL JOURNAL CONTENTS - OJ C 110 OF APRIL ...   \n",
       "\n",
       "                                                text  \n",
       "0  \\nThese are the leading stories in the South A...  \n",
       "1  \\n*\\n(Note - contents are displayed in reverse...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.DataFrame(documents)\n",
    "new_data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to pickle\n",
    "new_data.to_pickle('input/data_new.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
